from transformers import WhisperForConditionalGeneration, WhisperProcessor, VitsModel, AutoTokenizer
import librosa
import torch
import scipy.io.wavfile
import os

# Load models
asr_model = WhisperForConditionalGeneration.from_pretrained("whisperwave/Whisperwav-non-standard-big")
asr_processor = WhisperProcessor.from_pretrained("whisperwave/Whisperwav-non-standard-big")
tts_model = VitsModel.from_pretrained("facebook/mms-tts-aka")
tts_tokenizer = AutoTokenizer.from_pretrained("facebook/mms-tts-aka")


def generate_tts_audio(text: str, output_path: str = "output_tts.wav", speed: float = 1.0) -> str:
    inputs = tts_tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        output = tts_model(**inputs).waveform
    waveform = output.squeeze().cpu().numpy()

    if speed != 1.0:
        waveform = librosa.effects.time_stretch(waveform, speed)

    scipy.io.wavfile.write(output_path, rate=tts_model.config.sampling_rate, data=waveform)
    return output_path


def transcribe_twi(audio_path: str) -> str:
    audio, _ = librosa.load(audio_path, sr=16000)
    input_features = asr_processor(audio, sampling_rate=16000, return_tensors="pt").input_features
    with torch.no_grad():
        predicted_ids = asr_model.generate(input_features)
    transcription = asr_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription


def get_feedback(expected: str, actual: str) -> dict:
    expected_words = expected.strip().lower().split()
    actual_words = actual.strip().lower().split()

    feedback = []
    for idx, word in enumerate(expected_words):
        if idx < len(actual_words):
            color = "green" if word == actual_words[idx] else "yellow"
        else:
            color = "red"
        feedback.append({"word": word, "color": color})

    return {
        "expected": expected,
        "actual": actual,
        "feedback": feedback,
    }
